{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SImpdnZ3x_Pm",
        "HTqC5wVfHuFm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/11bus11/deep_learning_course_umu/blob/main/Erik_VF_5TF078_Laboration_4_om_spr%C3%A5kteknologi_(1_2)_Word2vec_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEeaOfHvBk_Q"
      },
      "source": [
        "# 5TF078 Deep Learning Course\n",
        "#Erik Vodopivec Forsman\n",
        "## Excercise 4 NLP - Word2Vec\n",
        "Created by Tomas Nordström, Umeå University\n",
        "\n",
        "Revisions:\n",
        "* 2023-12-03 Initial version with three different word2vec models (Builtin, Glove, GoogleNews) to be used from within GenSim /ToNo\n",
        "* 2023-12-06 Fix for gensim.downloader that now seems missing /Tomas\n",
        "* 2024-03-24 Updated tests for Kaggle. /Tomas\n",
        "* 2024-04-23 Included student calculation of similarity /Tomas\n",
        "* 2024-04-30 Fixed depreciated gensim glove2word2vec /Tomas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization"
      ],
      "metadata": {
        "id": "6t_XD9M13sMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "### Is this notebook running on Colab?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "### Is this notebook running on Kaggle?\n",
        "# Fool Kaggle into making kaggle_secrets avaiable\n",
        "try:\n",
        "    import kaggle_secrets\n",
        "except ImportError as e:\n",
        "    pass\n",
        "# Now we can test for Kaggle\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules"
      ],
      "metadata": {
        "id": "l5LvngJGNfWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"jax\" # Also jax,pytorch for Keras 3.0\n",
        "\n",
        "# Import Keras/TF libraries\n",
        "\n",
        "import keras\n",
        "print('Keras version:', keras.__version__)\n",
        "\n",
        "import tensorflow as tf\n",
        "print('TensorFlow version:', tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj4cihqkNftR",
        "outputId": "223fc733-7a95-4ce5-d3bb-074b05ff1f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras version: 2.15.0\n",
            "TensorFlow version: 2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper libraries\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import urllib\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader as gsapi\n",
        "\n",
        "# Matlab plotting\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "lZsRDD9kLn2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/37748105/how-to-use-progressbar-module-with-urlretrieve#53643011\n",
        "import progressbar\n",
        "class MyProgressBar():\n",
        "    def __init__(self):\n",
        "        self.pbar = None\n",
        "\n",
        "    def __call__(self, block_num, block_size, total_size):\n",
        "        if not self.pbar:\n",
        "            self.pbar=progressbar.ProgressBar(maxval=total_size)\n",
        "            self.pbar.start()\n",
        "\n",
        "        downloaded = block_num * block_size\n",
        "        if downloaded < total_size:\n",
        "            self.pbar.update(downloaded)\n",
        "        else:\n",
        "            self.pbar.finish()"
      ],
      "metadata": {
        "id": "HTGKKOrxJaKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Based on the Gensim framework\n",
        "\n",
        "Gensim Docs: https://radimrehurek.com/gensim/\n",
        "\n",
        "There are many ways to download models or data for word2vec models:\n",
        "1. The Gensim Builtin models (where 'glove-twitter-100' downloads 387 MB file)\n",
        "2. One of the Glove models (downloads a 822 MB zip file).\n",
        "3. GoogleNews based model"
      ],
      "metadata": {
        "id": "M6eK_hUWl29O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select one of Builtin, Glove, GoogleNews models to use\n",
        "model_to_use            = 'GoogleNews'\n",
        "model_to_use_if_builtin = 'glove-twitter-100'"
      ],
      "metadata": {
        "id": "sylqrAD8ytdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up a word2vec model"
      ],
      "metadata": {
        "id": "uyN1_RZZ9_Fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Look at what builtin Gensim models we can use\n",
        "\n",
        "Note that you need to select one out of the possible builtin models!"
      ],
      "metadata": {
        "id": "SImpdnZ3x_Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://radimrehurek.com/gensim/models/word2vec.html\n",
        "\n",
        "# Show all available models in gensim-data\n",
        "print(list(gsapi.info()['models'].keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aan4kJiDl2Gs",
        "outputId": "03557f5c-defa-426b-c250-38a33265a3c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model_to_use == 'Builtin':\n",
        "  w2v = gsapi.load(model_to_use_if_builtin)"
      ],
      "metadata": {
        "id": "9eBto-pamOuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using GloVe"
      ],
      "metadata": {
        "id": "HTqC5wVfHuFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download pre-trained GloVe embeddings (a 822 MB zip file).\n",
        "\n",
        "The archive contains text-encoded vectors of various sizes: 50-dimensional, 100-dimensional, 200-dimensional, 300-dimensional. We'll use the 100-D ones.\n",
        "\n"
      ],
      "metadata": {
        "id": "BL27KAmmHnk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a file to kerasdata, but first check if it already exist, now with progress bar\n",
        "DOWNLOADS_DIR = './kerasdata'\n",
        "os.makedirs(DOWNLOADS_DIR, exist_ok=True) # create dir if not exist\n",
        "\n",
        "if model_to_use == 'Glove':\n",
        "  url= 'https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip'\n",
        "  # Split on the rightmost / and take everything on the right side of that; up until '?'\n",
        "  name = url.rsplit('/', 1)[-1].split('?', 1)[0]\n",
        "  filename = os.path.join(DOWNLOADS_DIR, name)\n",
        "\n",
        "  # Download the file if it does not exist\n",
        "  if not os.path.isfile(filename):\n",
        "    print(f'Retrieving url: {url}', flush=True)\n",
        "    urllib.request.urlretrieve(url, filename, reporthook=MyProgressBar())\n",
        "  else:\n",
        "    print(f'Using local zip file: {filename}')\n",
        "\n",
        "  GLOVEDIR = os.path.join(DOWNLOADS_DIR, 'Glove')\n",
        "  path_to_glove_file =  os.path.join(GLOVEDIR, 'glove.6B.100d.txt')\n",
        "  if not os.path.isfile(path_to_glove_file):\n",
        "    ZipFile(filename).extractall(GLOVEDIR)\n",
        "\n",
        "  print(f'Using local glove file: {path_to_glove_file}')\n",
        "\n",
        "  # Now convert the Glove file to a gensim word2vec\n",
        "  # https://stackoverflow.com/questions/48743053/how-to-save-and-load-glove-models#51319383\n",
        "  GENSIMGLOVEFILE = os.path.join(GLOVEDIR,\"gensim_glove_vectors.txt\")\n",
        "\n",
        "  if not os.path.isfile(GENSIMGLOVEFILE):\n",
        "    # glove2word2vec(glove_input_file=path_to_glove_file, word2vec_output_file=GENSIMGLOVEFILE)\n",
        "    w2v = gensim.models.KeyedVectors.load_word2vec_format(path_to_glove_file, binary=False, no_header=True)\n",
        "  else:\n",
        "    w2v = gensim.models.KeyedVectors.load_word2vec_format(GENSIMGLOVEFILE, binary=False)\n"
      ],
      "metadata": {
        "id": "osnC1eKhJew3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using GoogleNews file"
      ],
      "metadata": {
        "id": "dGQ_Q68Uqcoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if model_to_use == 'GoogleNews':\n",
        "  # Download a file to kerasdata, but first check if it already exist, now with progress bar\n",
        "  DOWNLOADS_DIR = './kerasdata'\n",
        "  os.makedirs(DOWNLOADS_DIR, exist_ok=True) # create dir if not exist\n",
        "\n",
        "  url= 'https://git.ri.se/tomas.nordstrom/mldata/-/raw/main/GoogleNews-vectors-negative300.bin.gz?inline=false'\n",
        "  # Split on the rightmost / and take everything on the right side of that; up until '?'\n",
        "  name = url.rsplit('/', 1)[-1].split('?', 1)[0]\n",
        "  filename = os.path.join(DOWNLOADS_DIR, name)\n",
        "\n",
        "  # Download the file if it does not exist\n",
        "  if not os.path.isfile(filename):\n",
        "    print(f'Retrieving url: {url}', flush=True)\n",
        "    urllib.request.urlretrieve(url, filename, reporthook=MyProgressBar())\n",
        "  else:\n",
        "    print(f'Using local file: {filename}')\n",
        "\n",
        "  # Create the word2vec model\n",
        "  w2v = gensim.models.KeyedVectors.load_word2vec_format(filename, binary=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUZ2bpleqpra",
        "outputId": "e561991d-ac0b-432d-ed1a-dca2b92dd52d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving url: https://git.ri.se/tomas.nordstrom/mldata/-/raw/main/GoogleNews-vectors-negative300.bin.gz?inline=false\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% (1647046227 of 1647046227) |########| Elapsed Time: 0:01:18 Time:  0:01:18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments\n",
        "\n",
        "Check out https://radimrehurek.com/gensim/models/keyedvectors.html#what-can-i-do-with-word-vectors for examples what you can do with these vectors."
      ],
      "metadata": {
        "id": "1WjE4HHtp9xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First check out the word2vec model\n",
        "print(f'Model name {model_to_use}')\n",
        "print(f'Total number of words: {len(w2v.key_to_index)}') # Totalt antal ord\n",
        "print(f'Some example words: {list(w2v.key_to_index)[:20]}')\n",
        "embedding_len = len(w2v[0])\n",
        "print(f'Embedding vector length: {embedding_len}')\n",
        "\n",
        "# The check out a vector\n",
        "word1 = \"cat\"\n",
        "print(f'\\nLooking at \"{word1}\"')\n",
        "cat_vector = w2v[word1]\n",
        "print(f'Vector: {cat_vector[:10]}...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj5nxf1iv0Y4",
        "outputId": "72ba7413-ccd3-42c4-df18-05265a66c4f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model name GoogleNews\n",
            "Total number of words: 3000000\n",
            "Some example words: ['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said', 'was', 'the', 'at', 'not', 'as', 'it', 'be', 'from', 'by', 'are']\n",
            "Embedding vector length: 300\n",
            "\n",
            "Looking at \"cat\"\n",
            "Vector: [ 0.0123291   0.20410156 -0.28515625  0.21679688  0.11816406  0.08300781\n",
            "  0.04980469 -0.00952148  0.22070312 -0.12597656]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity\n",
        "\n",
        "Now we want to find similarity between word vectors and need to define a [similarity measure](https://en.wikipedia.org/wiki/Similarity_measure). In this exercise we will use [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) as a similarity measure.\n",
        "\n",
        "Your task is now to define a similarity function, and then generate three word vectors for “cat”, “cut”, and “dog” and compare the similarity between them using your similarity function.\n"
      ],
      "metadata": {
        "id": "ohoI1-jgGd1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def similarity(vec1,vec2):\n",
        "##################### TODO: YOUR CODE STARTS HERE #####################\n",
        "    cosine_dist = dot(vec1,vec2)/(norm(vec1)*norm(vec2))\n",
        "    return cosine_dist\n",
        "##################### TODO: YOUR CODE ENDS HERE #######################"
      ],
      "metadata": {
        "id": "AYSTfwsNHz9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cut_vector = w2v[\"cut\"]\n",
        "dog_vector = w2v[\"dog\"]\n",
        "\n",
        "##################### TODO: YOUR CODE STARTS HERE #####################\n",
        "# Compare the similarity between \"cat”, “cut”, and “dog”\n",
        "def calculate(vec1, vec2, vec3):\n",
        "  similarity1 = similarity(vec1, vec2)\n",
        "  similarity2 = similarity(vec1, vec3)\n",
        "  similarity3 = similarity(vec3, vec2)\n",
        "  print(\"result between 1 and 2:\", str(similarity1))\n",
        "  print(\"result between 1 and 3:\", str(similarity2))\n",
        "  print(\"result between 3 and 2:\", str(similarity3))\n",
        "\n",
        "calculate(cat_vector, cut_vector, dog_vector)\n",
        "\n",
        "\n",
        "##################### TODO: YOUR CODE ENDS HERE #######################"
      ],
      "metadata": {
        "id": "-CmmfLW2Ilfl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd14356e-da15-4fe1-bf59-8d22d32c97c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result between 1 and 2: 0.092555486\n",
            "result between 1 and 3: 0.76094574\n",
            "result between 3 and 2: 0.05553734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uppgift\n",
        "Skapa fler exempel och analysera dina resultat \\\n",
        "\n",
        "**Svar:** Testade att jämföra lite olika ord och fick bra resultat. T.ex fick kombinationen av rake och shovel en större likhet än pot kombinerat med rake eller shovel. Detta var förväntat då de båda förstnämnda är verktyg. Resterande ordkombiationer gav också väntade resultat."
      ],
      "metadata": {
        "id": "yEcHb3gT70p2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################### TODO: YOUR CODE STARTS HERE #####################\n",
        "# Create more examples\n",
        "print(\"no1\")\n",
        "eye_vector = w2v[\"eye\"]\n",
        "nose_vector = w2v[\"nose\"]\n",
        "toe_vector = w2v[\"toe\"]\n",
        "\n",
        "calculate(eye_vector, nose_vector, toe_vector)\n",
        "\n",
        "print(\"no2\")\n",
        "rake_vector = w2v[\"rake\"]\n",
        "shovel_vector = w2v[\"shovel\"]\n",
        "pot_vector = w2v[\"pot\"]\n",
        "\n",
        "calculate(rake_vector, shovel_vector, pot_vector)\n",
        "\n",
        "print(\"no3\")\n",
        "rabbit_vector = w2v[\"rabbit\"]\n",
        "\n",
        "calculate(dog_vector, cat_vector, rabbit_vector)\n",
        "\n",
        "##################### TODO: YOUR CODE ENDS HERE #######################"
      ],
      "metadata": {
        "id": "ax8oovHq9f2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3324470d-fc76-4f63-cb14-2a94bdf5264e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no1\n",
            "result between 1 and 2: 0.4343575\n",
            "result between 1 and 3: 0.2866297\n",
            "result between 3 and 2: 0.46895\n",
            "no2\n",
            "result between 1 and 2: 0.39041606\n",
            "result between 1 and 3: 0.32889664\n",
            "result between 3 and 2: 0.2366583\n",
            "no3\n",
            "result between 1 and 2: 0.76094574\n",
            "result between 1 and 3: 0.5868356\n",
            "result between 3 and 2: 0.62613827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding most similar\n",
        "We are also interested in finding the n closest words to a certain vector.\n",
        "\n",
        "We could do this by calculating a similarity to all vectors in the word2vec dictionary/matrix and then sort according to similarity score and take the top n-values.\n",
        "\n",
        "This is clearly doable, but for this course round we will use the most_similar methods on our w2v object.\n",
        "To get the ten most similar words to computer we do:\n",
        "`sims = w2v.most_similar('computer', topn=10)`\n",
        "\n",
        "\n",
        "We sometimes want to do operations with the vectors to do “king”-”man”+”woman”, and to support that we can use the two parameters: positive and negative:\n",
        "`w2v.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)`\n"
      ],
      "metadata": {
        "id": "OvXVoc_pImBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classic example\n",
        "result = w2v.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFD1RC4chvWj",
        "outputId": "2fe3c999-ab1e-4f0f-a15f-61dfe545378d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.7118193507194519)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##################### TODO: YOUR CODE STARTS HERE #####################\n",
        "# Create more examples\n",
        "result = w2v.most_similar(positive=['money', 'win'], negative=['game'], topn=1)\n",
        "print(result)\n",
        "\n",
        "\n",
        "##################### TODO: YOUR CODE ENDS HERE #######################"
      ],
      "metadata": {
        "id": "vaaKXVPk9XXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd4a7d0-a018-4674-a935-7be44841ec16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('funds', 0.5235512852668762)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uppgift\n",
        "Skapa fler \"räkneexempel\" och analysera dina resultat \\\n",
        "* house, city, countryside: apartment, 0.487262487411499 \\\n",
        "* falmily, cat, kids: feline, 0.5297148823738098 \\\n",
        "* school, kids, toy: students 0.6308646202087402 \\\n",
        "* building, adult, free: woodframe, 0.4492707848548889 \\\n",
        "* park, city, grass: town, 0.5479860901832581 \\\n",
        "* money, win, game: funds, 0.5235512852668762 \\\n",
        "\n",
        "Resultaten är rimliga med tanke på vad jag matat in. Märkte däremot att man ibland bara fick en pluralversion av ett av de inmatade orden. Man behövde alltså testa sig fram för att få relevanta resultat ifall man valde 'svårare' kombinationer."
      ],
      "metadata": {
        "id": "jZIXkPAh7kcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## There is actually a builtin similarity method"
      ],
      "metadata": {
        "id": "tA0NJz_c8M03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There is a built in similarity methods we can use instead of our own function\n",
        "word1 = 'cat'\n",
        "word2 = 'dog'\n",
        "print(f\"Similarity between {word1} and {word2} is {w2v.similarity(word1, word2)}\")\n",
        "\n",
        "word2 = 'cut'\n",
        "print(f\"Similarity between {word1} and {word2} is {w2v.similarity(word1, word2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnr43iynhOhf",
        "outputId": "3513b3f9-1f9a-4808-8098-629a09cf7bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between cat and dog is 0.760945737361908\n",
            "Similarity between cat and cut is 0.09255547821521759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uppgift\n",
        "Får du samma resultat med din likhetsfunktion som den i gensim inbyggda funktionen? \\\n",
        "**Svar:** Ja. Mina resultat är samma i alla fall som minst de först 4 decimalerna. Den inbyggda funktionen ger fler decimaler."
      ],
      "metadata": {
        "id": "JHJRWxg48AWJ"
      }
    }
  ]
}